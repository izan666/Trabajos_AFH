<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=dpiI8CyVsrzWsJLBFKehGpqJGWBNUW55wZAMyeVJrODnz3Vw3AFjQz6HdLM2ql0__-0gj9b5oU8OrQs51UKUORWvekCHwsTPUjMYQPqO2Fo);ol{margin:0;padding:0}table td,table th{padding:0}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c17{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right;height:11pt}.c3{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c13{color:#434343;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Roboto Mono";font-style:normal}.c14{font-weight:500;vertical-align:baseline;font-size:14pt;font-family:"Roboto";font-style:normal}.c1{font-weight:400;vertical-align:baseline;font-size:12pt;font-family:"Roboto";font-style:normal}.c15{font-weight:400;vertical-align:baseline;font-size:12pt;font-family:"Roboto Mono";font-style:normal}.c7{font-weight:700;vertical-align:baseline;font-size:17pt;font-family:"Roboto";font-style:normal}.c10{font-weight:400;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c16{vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c6{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c23{vertical-align:baseline;font-size:22pt;font-family:"Roboto Mono";font-style:italic}.c18{vertical-align:baseline;font-size:18pt;font-family:"Roboto Mono";font-style:normal}.c19{vertical-align:baseline;font-size:17pt;font-family:"Roboto Mono";font-style:normal}.c22{font-size:18pt;font-weight:400;font-family:"Roboto Mono"}.c21{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c5{color:inherit;text-decoration:inherit}.c0{color:#000000;text-decoration:none}.c24{height:14pt}.c20{height:11pt}.c12{margin-left:18pt}.c11{font-weight:700}.c9{page-break-after:avoid}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:0pt;color:#000000;font-weight:700;font-size:17pt;padding-bottom:0pt;font-family:"Roboto";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:500;font-size:14pt;padding-bottom:6pt;font-family:"Roboto";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c21 doc-content"><p class="c2"><span class="c0 c11 c23">Cuantizaci&oacute;n</span></p><p class="c2 c20"><span class="c0 c11 c19"></span></p><p class="c2"><span class="c13 c11">Capturas por: Ismael Herrero de la Torre</span></p><p class="c2 c20"><span class="c13 c11"></span></p><p class="c2"><span class="c11 c13">Desarrollo del documento : Izan Morcillo Mart&iacute;n</span></p><p class="c2 c20"><span class="c19 c0 c11"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 795.91px; height: 532.50px;"><img alt="" src="images/image10.png" style="width: 795.91px; height: 532.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c20"><span class="c19 c0 c11"></span></p><p class="c2 c20"><span class="c19 c0 c11"></span></p><p class="c4"><span class="c19 c0 c11"></span></p><p class="c2"><span class="c0 c11 c18">&Iacute;ndice</span></p><p class="c8"><span class="c22">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c8"><span class="c15 c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c6"><span class="c0 c11"><a class="c5" href="#h.rkh5fw4sd9ja">1. &iquest;Qu&eacute; es la cuantizaci&oacute;n en IA?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span class="c11"><a class="c5" href="#h.rkh5fw4sd9ja">3</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.20t7of8oh9hj">Definici&oacute;n t&eacute;cnica&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.20t7of8oh9hj">3</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.9qk98af0b6y6">&iquest;Qu&eacute; implica esto?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.9qk98af0b6y6">3</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.dyxachxsd6w8">Objetivo principal&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.dyxachxsd6w8">4</a></span></p><p class="c6"><span class="c0 c11"><a class="c5" href="#h.95kqkm2gnfj3">2. &iquest;Por qu&eacute; es necesaria?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span class="c11"><a class="c5" href="#h.95kqkm2gnfj3">4</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.us5l19ark51t">Contexto pr&aacute;ctico&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.us5l19ark51t">4</a></span></p><p class="c6"><span class="c0 c11"><a class="c5" href="#h.1fi6iycf46r8">3. Tipos de cuantizaci&oacute;n (visi&oacute;n general)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span class="c11"><a class="c5" href="#h.1fi6iycf46r8">5</a></span></p><p class="c6"><span class="c0 c11"><a class="c5" href="#h.7p0oj045sitz">4. Proceso de creaci&oacute;n y cuantizaci&oacute;n del modelo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span class="c11"><a class="c5" href="#h.7p0oj045sitz">5</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.zh9e3mc19brh">Organizaci&oacute;n inicial del proyecto&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.zh9e3mc19brh">5</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.77c0devmjako">Desarrollo del script principal&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.77c0devmjako">6</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.q5xh0kamh6cs">Implementaci&oacute;n de la cuantizaci&oacute;n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.q5xh0kamh6cs">6</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.mm7j3g8shwvc">Instalaci&oacute;n de dependencias&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.mm7j3g8shwvc">7</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.qffaycxt00q5">Verificaci&oacute;n del modelo cuantizado&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.qffaycxt00q5">7</a></span></p><p class="c6"><span class="c0 c11"><a class="c5" href="#h.8z10vh193pci">5. Ventajas y Desventajas&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span class="c11"><a class="c5" href="#h.8z10vh193pci">13</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.ssygd1vqnog8">Ventajas&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.ssygd1vqnog8">13</a></span></p><p class="c6 c12"><span class="c0"><a class="c5" href="#h.ivgiae3idsl3">Desventajas&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span><a class="c5" href="#h.ivgiae3idsl3">14</a></span></p><p class="c6"><span class="c0 c11"><a class="c5" href="#h.bpkrdlm8ggeq">6. Comparaci&oacute;n con otras t&eacute;cnicas de compresi&oacute;n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span class="c11"><a class="c5" href="#h.bpkrdlm8ggeq">14</a></span></p><p class="c6"><span class="c0 c11"><a class="c5" href="#h.f2e0wqv2i99v">7. Conclusi&oacute;n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a></span><span class="c11"><a class="c5" href="#h.f2e0wqv2i99v">15</a></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c0 c15"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><p class="c4"><span class="c15 c0"></span></p><h1 class="c2 c9" id="h.rkh5fw4sd9ja"><span>1.</span><span class="c7 c0">&nbsp;&iquest;Qu&eacute; es la cuantizaci&oacute;n en IA?</span></h1><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">La cuantizaci&oacute;n en Inteligencia Artificial es una t&eacute;cnica de optimizaci&oacute;n que consiste en reducir la precisi&oacute;n num&eacute;rica de los par&aacute;metros (pesos) y activaciones de un modelo para disminuir su tama&ntilde;o y mejorar su eficiencia computacional.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Normalmente, los modelos de redes neuronales trabajan con n&uacute;meros en punto flotante de 32 bits (float32). La cuantizaci&oacute;n transforma estos valores en representaciones de menor precisi&oacute;n, como int8 (8 bits) o float16.</span></p><p class="c4"><span class="c1 c0"></span></p><h2 class="c3" id="h.20t7of8oh9hj"><span class="c14 c0">Definici&oacute;n t&eacute;cnica</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Es el proceso de mapear un conjunto continuo de valores (alta precisi&oacute;n) a un conjunto discreto m&aacute;s peque&ntilde;o (menor precisi&oacute;n), manteniendo el rendimiento del modelo lo m&aacute;s estable posible.</span></p><p class="c4"><span class="c1 c0"></span></p><h2 class="c3" id="h.9qk98af0b6y6"><span class="c14 c0">&iquest;Qu&eacute; implica esto?</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Reducci&oacute;n del tama&ntilde;o del modelo.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Menor uso de memoria.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Mayor velocidad de inferencia.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Menor consumo energ&eacute;tico.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Ejemplo sencillo</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Si un modelo ocupa 100 MB en float32, tras cuantizaci&oacute;n a int8 puede reducirse aproximadamente a 25 MB.</span></p><p class="c4"><span class="c1 c0"></span></p><h2 class="c3" id="h.dyxachxsd6w8"><span class="c14 c0">Objetivo principal</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Hacer que modelos grandes puedan ejecutarse en dispositivos con recursos limitados sin perder demasiada precisi&oacute;n.</span></p><p class="c4"><span class="c1 c0"></span></p><h1 class="c2 c9" id="h.95kqkm2gnfj3"><span>2. </span><span class="c0 c7">&iquest;Por qu&eacute; es necesaria?</span></h1><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Los modelos actuales de IA son cada vez m&aacute;s grandes y complejos. Organizaciones como OpenAI o Google DeepMind desarrollan modelos con miles de millones de par&aacute;metros.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Problemas actuales:</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Alto consumo de memoria.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Gran demanda de energ&iacute;a.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Costes elevados en infraestructura.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Dificultad para ejecutar modelos en m&oacute;viles o dispositivos edge.</span></p><p class="c4"><span class="c1 c0"></span></p><h2 class="c3" id="h.us5l19ark51t"><span class="c14 c0">Contexto pr&aacute;ctico</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">En dispositivos como smartphones, IoT o sistemas embebidos, no se dispone de GPUs potentes. La cuantizaci&oacute;n permite que estos modelos funcionen en hardware limitado sin necesidad de servidores externos.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Adem&aacute;s, reduce el impacto ambiental al disminuir el consumo energ&eacute;tico en centros de datos.</span></p><p class="c4"><span class="c1 c0"></span></p><h1 class="c2 c9" id="h.1fi6iycf46r8"><span>3.</span><span class="c7 c0">&nbsp;Tipos de cuantizaci&oacute;n (visi&oacute;n general)</span></h1><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Existen distintos tipos de cuantizaci&oacute;n:</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Post-Training Quantization (PTQ): se aplica despu&eacute;s del entrenamiento. Es m&aacute;s r&aacute;pida y sencilla, pero puede reducir algo la precisi&oacute;n.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Quantization Aware Training (QAT): el modelo se entrena teniendo en cuenta la cuantizaci&oacute;n desde el inicio, logrando mejores resultados.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Cuantizaci&oacute;n din&aacute;mica: convierte valores durante la ejecuci&oacute;n.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Cuantizaci&oacute;n est&aacute;tica: requiere calibraci&oacute;n previa con datos representativos.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Cada tipo implica un equilibrio diferente entre precisi&oacute;n y eficiencia.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c4"><span class="c1 c0"></span></p><p class="c4"><span class="c1 c0"></span></p><h1 class="c2 c9" id="h.7p0oj045sitz"><span>4. </span><span class="c7 c0">Proceso de creaci&oacute;n y cuantizaci&oacute;n del modelo</span></h1><h2 class="c3" id="h.zh9e3mc19brh"><span class="c14 c0">&nbsp;Organizaci&oacute;n inicial del proyecto</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">En primer lugar, organic&eacute; la estructura de carpetas del proyecto. Cre&eacute; un directorio espec&iacute;fico para la cuantizaci&oacute;n y trabaj&eacute; dentro de un entorno virtual (venv) para aislar las dependencias.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Dentro del proyecto inclu&iacute; los archivos principales:</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">main.py</span></p><p class="c4"><span class="c0 c1"></span></p><p class="c8"><span class="c1 c0">modelo_cuantizado_onnx.py</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Carpeta del entorno virtual</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Con esto me asegur&eacute; de mantener el proyecto ordenado y evitar conflictos con otras librer&iacute;as instaladas en el sistema.</span></p><h2 class="c3 c24" id="h.tgvai5dgywaq"><span class="c14 c0"></span></h2><h2 class="c3" id="h.77c0devmjako"><span class="c14 c0">&nbsp;Desarrollo del script principal</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">A continuaci&oacute;n, desarroll&eacute; el archivo main.py, donde implement&eacute; la l&oacute;gica principal del modelo.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">En este script:</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Import&eacute; las librer&iacute;as necesarias como torch, transformers y otras dependencias.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Cargu&eacute; un modelo preentrenado utilizando AutoTokenizer y AutoModelForSequenceClassification.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Defin&iacute; el texto de prueba para realizar la inferencia.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Tokenic&eacute; el texto de entrada utilizando el tokenizer correspondiente.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Ejecut&eacute; la inferencia con el modelo en modo evaluaci&oacute;n (model.eval()).</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Proces&eacute; la salida (logits) y obtuve la predicci&oacute;n final.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Mostr&eacute; por pantalla la clase predicha.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">En esta fase valid&eacute; que el modelo funcionara correctamente antes de aplicar la cuantizaci&oacute;n.</span></p><p class="c4"><span class="c1 c0"></span></p><h2 class="c3" id="h.q5xh0kamh6cs"><span class="c14 c0">&nbsp;Implementaci&oacute;n de la cuantizaci&oacute;n</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Despu&eacute;s, en el archivo modelo_cuantizado_onnx.py, implement&eacute; el proceso de exportaci&oacute;n y cuantizaci&oacute;n del modelo.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">En este script:</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Cargu&eacute; nuevamente el modelo preentrenado.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Prepar&eacute; una entrada de ejemplo (dummy input) necesaria para exportar el modelo.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Export&eacute; el modelo a formato ONNX utilizando torch.onnx.export.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Especifiqu&eacute;:</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">El nombre del archivo de salida .onnx</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Los nombres de entrada y salida</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">El uso de ejes din&aacute;micos para permitir diferentes tama&ntilde;os de batch</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Apliqu&eacute; la cuantizaci&oacute;n din&aacute;mica utilizando las herramientas de ONNX Runtime.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Guard&eacute; el modelo cuantizado en un nuevo archivo.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Este proceso permiti&oacute; reducir el tama&ntilde;o del modelo y optimizar su rendimiento en inferencia.</span></p><p class="c4"><span class="c1 c0"></span></p><h2 class="c3" id="h.mm7j3g8shwvc"><span class="c14 c0">&nbsp;Instalaci&oacute;n de dependencias</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Para que todo el entorno funcionara correctamente, instal&eacute; las siguientes librer&iacute;as:</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">onnx</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">onnxruntime</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">onnxruntime-tools</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">torch</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">transformers</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">huggingface_hub</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">accelerate</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">La instalaci&oacute;n la realic&eacute; mediante pip install, asegur&aacute;ndome de que todas las dependencias necesarias para exportaci&oacute;n y cuantizaci&oacute;n estuvieran disponibles.</span></p><p class="c4"><span class="c1 c0"></span></p><h2 class="c3" id="h.qffaycxt00q5"><span>&nbsp;Verificaci&oacute;n del modelo cuantizado</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Finalmente, ejecut&eacute; el modelo cuantizado y comprob&eacute;:</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Que el archivo ONNX se hab&iacute;a generado correctamente.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Que el modelo reducido funcionaba sin errores.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Que la inferencia produc&iacute;a resultados coherentes.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Tambi&eacute;n verifiqu&eacute; por consola los logs de ejecuci&oacute;n para asegurarme de que no hubiera fallos en la exportaci&oacute;n ni en la cuantizaci&oacute;n.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Conclusi&oacute;n del proceso</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">En resumen, primero valid&eacute; el modelo original, despu&eacute;s lo export&eacute; a ONNX y finalmente apliqu&eacute; cuantizaci&oacute;n din&aacute;mica para optimizarlo. Durante todo el proceso me asegur&eacute; de mantener un entorno controlado, verificar cada paso y comprobar el funcionamiento final del modelo cuantizado.</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 733.33px;"><img alt="" src="images/image2.png" style="width: 601.70px; height: 733.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 177.33px;"><img alt="" src="images/image3.png" style="width: 601.70px; height: 177.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 349.00px; height: 149.00px;"><img alt="" src="images/image5.png" style="width: 349.00px; height: 149.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 118.67px;"><img alt="" src="images/image7.png" style="width: 601.70px; height: 118.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 238.67px;"><img alt="" src="images/image4.png" style="width: 601.70px; height: 238.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 146.67px;"><img alt="" src="images/image9.png" style="width: 601.70px; height: 146.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 169.33px;"><img alt="" src="images/image1.png" style="width: 601.70px; height: 169.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 429.33px;"><img alt="" src="images/image11.png" style="width: 601.70px; height: 429.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 308.00px;"><img alt="" src="images/image6.png" style="width: 601.70px; height: 308.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 705.33px;"><img alt="" src="images/image8.png" style="width: 601.70px; height: 705.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c1 c0"></span></p><h1 class="c2 c9" id="h.8z10vh193pci"><span class="c7 c0">&nbsp;5. Ventajas y Desventajas</span></h1><h2 class="c3" id="h.ssygd1vqnog8"><span class="c0 c14">Ventajas</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Reducci&oacute;n significativa del tama&ntilde;o del modelo</span></p><p class="c8"><span class="c1 c0">Puede reducirse hasta 4 veces o m&aacute;s.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Mayor velocidad de inferencia</span></p><p class="c8"><span class="c1 c0">Las operaciones con enteros son m&aacute;s r&aacute;pidas que con float32.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Menor consumo energ&eacute;tico</span></p><p class="c8"><span class="c1 c0">Fundamental para dispositivos m&oacute;viles y centros de datos.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Facilita el despliegue en hardware limitado</span></p><p class="c8"><span class="c1 c0">Permite ejecutar modelos en microcontroladores o dispositivos edge.</span></p><p class="c4"><span class="c1 c0"></span></p><h2 class="c3" id="h.ivgiae3idsl3"><span class="c14 c0">Desventajas</span></h2><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">P&eacute;rdida de precisi&oacute;n</span></p><p class="c8"><span class="c1 c0">Puede disminuir ligeramente el rendimiento del modelo.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Complejidad t&eacute;cnica</span></p><p class="c8"><span class="c1 c0">Implementar cuantizaci&oacute;n avanzada requiere conocimientos matem&aacute;ticos y t&eacute;cnicos.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">No todos los modelos responden igual</span></p><p class="c8"><span class="c1 c0">Algunos modelos son m&aacute;s sensibles a la reducci&oacute;n de precisi&oacute;n.</span></p><p class="c4"><span class="c1 c0"></span></p><h1 class="c2 c9" id="h.bpkrdlm8ggeq"><span class="c7 c0">&nbsp;6. Comparaci&oacute;n con otras t&eacute;cnicas de compresi&oacute;n </span></h1><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">La cuantizaci&oacute;n no es la &uacute;nica t&eacute;cnica para reducir modelos:</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Pruning: elimina conexiones o pesos poco importantes.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Distillation (Knowledge Distillation): un modelo peque&ntilde;o aprende de uno grande.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Diferencia principal:</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">La cuantizaci&oacute;n reduce la precisi&oacute;n num&eacute;rica.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">El pruning reduce la estructura.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">La distillation reduce la complejidad aprendiendo de un modelo maestro.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">En muchos casos se combinan varias t&eacute;cnicas para obtener mejores resultados.</span></p><p class="c4"><span class="c1 c0"></span></p><h1 class="c2 c9" id="h.f2e0wqv2i99v"><span>7.</span><span class="c7 c0">&nbsp;Conclusi&oacute;n</span></h1><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">La cuantizaci&oacute;n es una t&eacute;cnica fundamental en la optimizaci&oacute;n de modelos de Inteligencia Artificial. Permite reducir el tama&ntilde;o, acelerar la inferencia y disminuir el consumo energ&eacute;tico, facilitando el despliegue en dispositivos con recursos limitados.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">Aunque puede implicar una ligera p&eacute;rdida de precisi&oacute;n, sus beneficios en eficiencia la convierten en una herramienta clave en el desarrollo moderno de IA.</span></p><p class="c4"><span class="c1 c0"></span></p><p class="c8"><span class="c1 c0">En un contexto donde los modelos son cada vez m&aacute;s grandes y costosos, la cuantizaci&oacute;n representa una soluci&oacute;n estrat&eacute;gica para hacer la Inteligencia Artificial m&aacute;s accesible, sostenible y escalable.</span></p><p class="c4"><span class="c1 c0"></span></p><div><p class="c17"><span class="c10 c0"></span></p></div></body></html>