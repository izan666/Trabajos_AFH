<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=dpiI8CyVsrzWsJLBFKehGpqJGWBNUW55wZAMyeVJrODnz3Vw3AFjQz6HdLM2ql0__-0gj9b5oU8OrQs51UKUORWvekCHwsTPUjMYQPqO2Fo);ol{margin:0;padding:0}table td,table th{padding:0}.c4{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Roboto";font-style:normal}.c18{color:#434343;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Roboto Mono";font-style:normal}.c6{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c9{color:#000000;font-weight:500;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Roboto";font-style:normal}.c20{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:22pt;font-family:"Roboto Mono";font-style:italic}.c7{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17pt;font-family:"Roboto Mono";font-style:normal}.c15{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Roboto Mono";font-style:normal}.c8{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:17pt;font-family:"Roboto";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Roboto Mono";font-style:normal}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right}.c22{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c12{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c14{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c10{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c16{font-size:18pt;font-weight:400;font-family:"Roboto Mono"}.c17{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c0{color:inherit;text-decoration:inherit}.c21{height:14pt}.c13{page-break-after:avoid}.c19{font-weight:400}.c5{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:0pt;color:#000000;font-weight:700;font-size:17pt;padding-bottom:0pt;font-family:"Roboto";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:500;font-size:14pt;padding-bottom:6pt;font-family:"Roboto";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c17 doc-content"><p class="c1"><span class="c20">Cuantizaci&oacute;n</span></p><p class="c1 c5"><span class="c7"></span></p><p class="c1"><span class="c18">Capturas por: Ismael Herrero de la Torre</span></p><p class="c1 c5"><span class="c18"></span></p><p class="c1"><span class="c18">Desarrollo del documento : Izan Morcillo Mart&iacute;n</span></p><p class="c1 c5"><span class="c7"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 795.91px; height: 532.50px;"><img alt="" src="images/image10.png" style="width: 795.91px; height: 532.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c5"><span class="c7"></span></p><p class="c1 c5"><span class="c7"></span></p><p class="c3 c5"><span class="c7"></span></p><p class="c1"><span class="c15">&Iacute;ndice</span></p><p class="c3"><span class="c16">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3"><span class="c11">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c3"><span class="c11">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c14"><span class="c10 c19"><a class="c0" href="#h.rkh5fw4sd9ja">1. &iquest;Qu&eacute; es la cuantizaci&oacute;n en IA?</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.20t7of8oh9hj">Definici&oacute;n t&eacute;cnica</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.9qk98af0b6y6">&iquest;Qu&eacute; implica esto?</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.dyxachxsd6w8">Objetivo principal</a></span></p><p class="c14"><span class="c10 c19"><a class="c0" href="#h.95kqkm2gnfj3">2. &iquest;Por qu&eacute; es necesaria?</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.us5l19ark51t">Contexto pr&aacute;ctico</a></span></p><p class="c14"><span class="c10 c19"><a class="c0" href="#h.1fi6iycf46r8">3. Tipos de cuantizaci&oacute;n (visi&oacute;n general)</a></span></p><p class="c14"><span class="c10 c19"><a class="c0" href="#h.7p0oj045sitz">4. Proceso de creaci&oacute;n y cuantizaci&oacute;n del modelo</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.zh9e3mc19brh">Organizaci&oacute;n inicial del proyecto</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.77c0devmjako">Desarrollo del script principal</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.q5xh0kamh6cs">Implementaci&oacute;n de la cuantizaci&oacute;n</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.mm7j3g8shwvc">Instalaci&oacute;n de dependencias</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.qffaycxt00q5">Verificaci&oacute;n del modelo cuantizado</a></span></p><p class="c14"><span class="c10 c19"><a class="c0" href="#h.8z10vh193pci">5. Ventajas y Desventajas</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.ssygd1vqnog8">Ventajas</a></span></p><p class="c12"><span class="c10"><a class="c0" href="#h.ivgiae3idsl3">Desventajas</a></span></p><p class="c14"><span class="c10 c19"><a class="c0" href="#h.bpkrdlm8ggeq">6. Comparaci&oacute;n con otras t&eacute;cnicas de compresi&oacute;n</a></span></p><p class="c14"><span class="c10 c19"><a class="c0" href="#h.f2e0wqv2i99v">7. Conclusi&oacute;n</a></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><p class="c3 c5"><span class="c11"></span></p><h1 class="c1 c13" id="h.rkh5fw4sd9ja"><span>1.</span><span class="c8">&nbsp;&iquest;Qu&eacute; es la cuantizaci&oacute;n en IA?</span></h1><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">La cuantizaci&oacute;n en Inteligencia Artificial es una t&eacute;cnica de optimizaci&oacute;n que consiste en reducir la precisi&oacute;n num&eacute;rica de los par&aacute;metros (pesos) y activaciones de un modelo para disminuir su tama&ntilde;o y mejorar su eficiencia computacional.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Normalmente, los modelos de redes neuronales trabajan con n&uacute;meros en punto flotante de 32 bits (float32). La cuantizaci&oacute;n transforma estos valores en representaciones de menor precisi&oacute;n, como int8 (8 bits) o float16.</span></p><p class="c3 c5"><span class="c2"></span></p><h2 class="c6" id="h.20t7of8oh9hj"><span class="c9">Definici&oacute;n t&eacute;cnica</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Es el proceso de mapear un conjunto continuo de valores (alta precisi&oacute;n) a un conjunto discreto m&aacute;s peque&ntilde;o (menor precisi&oacute;n), manteniendo el rendimiento del modelo lo m&aacute;s estable posible.</span></p><p class="c3 c5"><span class="c2"></span></p><h2 class="c6" id="h.9qk98af0b6y6"><span class="c9">&iquest;Qu&eacute; implica esto?</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Reducci&oacute;n del tama&ntilde;o del modelo.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Menor uso de memoria.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Mayor velocidad de inferencia.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Menor consumo energ&eacute;tico.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Ejemplo sencillo</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Si un modelo ocupa 100 MB en float32, tras cuantizaci&oacute;n a int8 puede reducirse aproximadamente a 25 MB.</span></p><p class="c3 c5"><span class="c2"></span></p><h2 class="c6" id="h.dyxachxsd6w8"><span class="c9">Objetivo principal</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Hacer que modelos grandes puedan ejecutarse en dispositivos con recursos limitados sin perder demasiada precisi&oacute;n.</span></p><p class="c3 c5"><span class="c2"></span></p><h1 class="c1 c13" id="h.95kqkm2gnfj3"><span>2. </span><span class="c8">&iquest;Por qu&eacute; es necesaria?</span></h1><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Los modelos actuales de IA son cada vez m&aacute;s grandes y complejos. Organizaciones como OpenAI o Google DeepMind desarrollan modelos con miles de millones de par&aacute;metros.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Problemas actuales:</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Alto consumo de memoria.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Gran demanda de energ&iacute;a.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Costes elevados en infraestructura.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Dificultad para ejecutar modelos en m&oacute;viles o dispositivos edge.</span></p><p class="c3 c5"><span class="c2"></span></p><h2 class="c6" id="h.us5l19ark51t"><span class="c9">Contexto pr&aacute;ctico</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">En dispositivos como smartphones, IoT o sistemas embebidos, no se dispone de GPUs potentes. La cuantizaci&oacute;n permite que estos modelos funcionen en hardware limitado sin necesidad de servidores externos.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Adem&aacute;s, reduce el impacto ambiental al disminuir el consumo energ&eacute;tico en centros de datos.</span></p><p class="c3 c5"><span class="c2"></span></p><h1 class="c1 c13" id="h.1fi6iycf46r8"><span>3.</span><span class="c8">&nbsp;Tipos de cuantizaci&oacute;n (visi&oacute;n general)</span></h1><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Existen distintos tipos de cuantizaci&oacute;n:</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Post-Training Quantization (PTQ): se aplica despu&eacute;s del entrenamiento. Es m&aacute;s r&aacute;pida y sencilla, pero puede reducir algo la precisi&oacute;n.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Quantization Aware Training (QAT): el modelo se entrena teniendo en cuenta la cuantizaci&oacute;n desde el inicio, logrando mejores resultados.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Cuantizaci&oacute;n din&aacute;mica: convierte valores durante la ejecuci&oacute;n.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Cuantizaci&oacute;n est&aacute;tica: requiere calibraci&oacute;n previa con datos representativos.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Cada tipo implica un equilibrio diferente entre precisi&oacute;n y eficiencia.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3 c5"><span class="c2"></span></p><h1 class="c1 c13" id="h.7p0oj045sitz"><span>4. </span><span class="c8">Proceso de creaci&oacute;n y cuantizaci&oacute;n del modelo</span></h1><h2 class="c6" id="h.zh9e3mc19brh"><span class="c9">&nbsp;Organizaci&oacute;n inicial del proyecto</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">En primer lugar, organic&eacute; la estructura de carpetas del proyecto. Cre&eacute; un directorio espec&iacute;fico para la cuantizaci&oacute;n y trabaj&eacute; dentro de un entorno virtual (venv) para aislar las dependencias.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Dentro del proyecto inclu&iacute; los archivos principales:</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">main.py</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">modelo_cuantizado_onnx.py</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Carpeta del entorno virtual</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Con esto me asegur&eacute; de mantener el proyecto ordenado y evitar conflictos con otras librer&iacute;as instaladas en el sistema.</span></p><h2 class="c6 c21" id="h.tgvai5dgywaq"><span class="c9"></span></h2><h2 class="c6" id="h.77c0devmjako"><span class="c9">&nbsp;Desarrollo del script principal</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">A continuaci&oacute;n, desarroll&eacute; el archivo main.py, donde implement&eacute; la l&oacute;gica principal del modelo.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">En este script:</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Import&eacute; las librer&iacute;as necesarias como torch, transformers y otras dependencias.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Cargu&eacute; un modelo preentrenado utilizando AutoTokenizer y AutoModelForSequenceClassification.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Defin&iacute; el texto de prueba para realizar la inferencia.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Tokenic&eacute; el texto de entrada utilizando el tokenizer correspondiente.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Ejecut&eacute; la inferencia con el modelo en modo evaluaci&oacute;n (model.eval()).</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Proces&eacute; la salida (logits) y obtuve la predicci&oacute;n final.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Mostr&eacute; por pantalla la clase predicha.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">En esta fase valid&eacute; que el modelo funcionara correctamente antes de aplicar la cuantizaci&oacute;n.</span></p><p class="c3 c5"><span class="c2"></span></p><h2 class="c6" id="h.q5xh0kamh6cs"><span class="c9">&nbsp;Implementaci&oacute;n de la cuantizaci&oacute;n</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Despu&eacute;s, en el archivo modelo_cuantizado_onnx.py, implement&eacute; el proceso de exportaci&oacute;n y cuantizaci&oacute;n del modelo.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">En este script:</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Cargu&eacute; nuevamente el modelo preentrenado.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Prepar&eacute; una entrada de ejemplo (dummy input) necesaria para exportar el modelo.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Export&eacute; el modelo a formato ONNX utilizando torch.onnx.export.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Especifiqu&eacute;:</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">El nombre del archivo de salida .onnx</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Los nombres de entrada y salida</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">El uso de ejes din&aacute;micos para permitir diferentes tama&ntilde;os de batch</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Apliqu&eacute; la cuantizaci&oacute;n din&aacute;mica utilizando las herramientas de ONNX Runtime.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Guard&eacute; el modelo cuantizado en un nuevo archivo.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Este proceso permiti&oacute; reducir el tama&ntilde;o del modelo y optimizar su rendimiento en inferencia.</span></p><p class="c3 c5"><span class="c2"></span></p><h2 class="c6" id="h.mm7j3g8shwvc"><span class="c9">&nbsp;Instalaci&oacute;n de dependencias</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Para que todo el entorno funcionara correctamente, instal&eacute; las siguientes librer&iacute;as:</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">onnx</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">onnxruntime</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">onnxruntime-tools</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">torch</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">transformers</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">huggingface_hub</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">accelerate</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">La instalaci&oacute;n la realic&eacute; mediante pip install, asegur&aacute;ndome de que todas las dependencias necesarias para exportaci&oacute;n y cuantizaci&oacute;n estuvieran disponibles.</span></p><p class="c3 c5"><span class="c2"></span></p><h2 class="c6" id="h.qffaycxt00q5"><span>&nbsp;Verificaci&oacute;n del modelo cuantizado</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Finalmente, ejecut&eacute; el modelo cuantizado y comprob&eacute;:</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Que el archivo ONNX se hab&iacute;a generado correctamente.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Que el modelo reducido funcionaba sin errores.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Que la inferencia produc&iacute;a resultados coherentes.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Tambi&eacute;n verifiqu&eacute; por consola los logs de ejecuci&oacute;n para asegurarme de que no hubiera fallos en la exportaci&oacute;n ni en la cuantizaci&oacute;n.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Conclusi&oacute;n del proceso</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">En resumen, primero valid&eacute; el modelo original, despu&eacute;s lo export&eacute; a ONNX y finalmente apliqu&eacute; cuantizaci&oacute;n din&aacute;mica para optimizarlo. Durante todo el proceso me asegur&eacute; de mantener un entorno controlado, verificar cada paso y comprobar el funcionamiento final del modelo cuantizado.</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 733.33px;"><img alt="" src="images/image2.png" style="width: 601.70px; height: 733.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 177.33px;"><img alt="" src="images/image3.png" style="width: 601.70px; height: 177.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 349.00px; height: 149.00px;"><img alt="" src="images/image5.png" style="width: 349.00px; height: 149.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 118.67px;"><img alt="" src="images/image7.png" style="width: 601.70px; height: 118.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 238.67px;"><img alt="" src="images/image4.png" style="width: 601.70px; height: 238.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 146.67px;"><img alt="" src="images/image9.png" style="width: 601.70px; height: 146.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 169.33px;"><img alt="" src="images/image1.png" style="width: 601.70px; height: 169.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 429.33px;"><img alt="" src="images/image11.png" style="width: 601.70px; height: 429.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 308.00px;"><img alt="" src="images/image6.png" style="width: 601.70px; height: 308.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 705.33px;"><img alt="" src="images/image8.png" style="width: 601.70px; height: 705.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c5"><span class="c2"></span></p><h1 class="c1 c13" id="h.8z10vh193pci"><span class="c8">&nbsp;5. Ventajas y Desventajas</span></h1><h2 class="c6" id="h.ssygd1vqnog8"><span class="c9">Ventajas</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Reducci&oacute;n significativa del tama&ntilde;o del modelo</span></p><p class="c3"><span class="c2">Puede reducirse hasta 4 veces o m&aacute;s.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Mayor velocidad de inferencia</span></p><p class="c3"><span class="c2">Las operaciones con enteros son m&aacute;s r&aacute;pidas que con float32.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Menor consumo energ&eacute;tico</span></p><p class="c3"><span class="c2">Fundamental para dispositivos m&oacute;viles y centros de datos.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Facilita el despliegue en hardware limitado</span></p><p class="c3"><span class="c2">Permite ejecutar modelos en microcontroladores o dispositivos edge.</span></p><p class="c3 c5"><span class="c2"></span></p><h2 class="c6" id="h.ivgiae3idsl3"><span class="c9">Desventajas</span></h2><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">P&eacute;rdida de precisi&oacute;n</span></p><p class="c3"><span class="c2">Puede disminuir ligeramente el rendimiento del modelo.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Complejidad t&eacute;cnica</span></p><p class="c3"><span class="c2">Implementar cuantizaci&oacute;n avanzada requiere conocimientos matem&aacute;ticos y t&eacute;cnicos.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">No todos los modelos responden igual</span></p><p class="c3"><span class="c2">Algunos modelos son m&aacute;s sensibles a la reducci&oacute;n de precisi&oacute;n.</span></p><p class="c3 c5"><span class="c2"></span></p><h1 class="c1 c13" id="h.bpkrdlm8ggeq"><span class="c8">&nbsp;6. Comparaci&oacute;n con otras t&eacute;cnicas de compresi&oacute;n </span></h1><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">La cuantizaci&oacute;n no es la &uacute;nica t&eacute;cnica para reducir modelos:</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Pruning: elimina conexiones o pesos poco importantes.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Distillation (Knowledge Distillation): un modelo peque&ntilde;o aprende de uno grande.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Diferencia principal:</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">La cuantizaci&oacute;n reduce la precisi&oacute;n num&eacute;rica.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">El pruning reduce la estructura.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">La distillation reduce la complejidad aprendiendo de un modelo maestro.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">En muchos casos se combinan varias t&eacute;cnicas para obtener mejores resultados.</span></p><p class="c3 c5"><span class="c2"></span></p><h1 class="c1 c13" id="h.f2e0wqv2i99v"><span>7.</span><span class="c8">&nbsp;Conclusi&oacute;n</span></h1><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">La cuantizaci&oacute;n es una t&eacute;cnica fundamental en la optimizaci&oacute;n de modelos de Inteligencia Artificial. Permite reducir el tama&ntilde;o, acelerar la inferencia y disminuir el consumo energ&eacute;tico, facilitando el despliegue en dispositivos con recursos limitados.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">Aunque puede implicar una ligera p&eacute;rdida de precisi&oacute;n, sus beneficios en eficiencia la convierten en una herramienta clave en el desarrollo moderno de IA.</span></p><p class="c3 c5"><span class="c2"></span></p><p class="c3"><span class="c2">En un contexto donde los modelos son cada vez m&aacute;s grandes y costosos, la cuantizaci&oacute;n representa una soluci&oacute;n estrat&eacute;gica para hacer la Inteligencia Artificial m&aacute;s accesible, sostenible y escalable.</span></p><p class="c3 c5"><span class="c2"></span></p><div><p class="c5 c23"><span class="c19 c22"></span></p></div></body></html>