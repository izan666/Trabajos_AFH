<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=fND5XPYKrF2tQDwwfWZJI_esZW2xOQ-xsNqO47m55DA);.lst-kix_bnqoz09lekie-0>li:before{content:"\0025cf   "}.lst-kix_le5rw7vfsnpe-8>li:before{content:"\0025a0   "}.lst-kix_bnqoz09lekie-2>li:before{content:"\0025a0   "}.lst-kix_le5rw7vfsnpe-6>li:before{content:"\0025cf   "}.lst-kix_bnqoz09lekie-1>li:before{content:"\0025cb   "}.lst-kix_le5rw7vfsnpe-7>li:before{content:"\0025cb   "}ul.lst-kix_quzgpc4u83ik-2{list-style-type:none}.lst-kix_ph3vtg5kepvs-3>li:before{content:"\0025cf   "}ul.lst-kix_quzgpc4u83ik-1{list-style-type:none}.lst-kix_bnqoz09lekie-6>li:before{content:"\0025cf   "}.lst-kix_bnqoz09lekie-7>li:before{content:"\0025cb   "}ul.lst-kix_quzgpc4u83ik-0{list-style-type:none}ul.lst-kix_quzgpc4u83ik-6{list-style-type:none}.lst-kix_ph3vtg5kepvs-1>li:before{content:"\0025cb   "}.lst-kix_ph3vtg5kepvs-2>li:before{content:"\0025a0   "}ul.lst-kix_quzgpc4u83ik-5{list-style-type:none}ul.lst-kix_quzgpc4u83ik-4{list-style-type:none}ul.lst-kix_quzgpc4u83ik-3{list-style-type:none}.lst-kix_bnqoz09lekie-3>li:before{content:"\0025cf   "}.lst-kix_ph3vtg5kepvs-0>li:before{content:"\0025cf   "}ul.lst-kix_quzgpc4u83ik-8{list-style-type:none}.lst-kix_bnqoz09lekie-4>li:before{content:"\0025cb   "}ul.lst-kix_quzgpc4u83ik-7{list-style-type:none}.lst-kix_bnqoz09lekie-5>li:before{content:"\0025a0   "}.lst-kix_6eeexwh7rz-4>li:before{content:"\0025cb   "}.lst-kix_quzgpc4u83ik-8>li:before{content:"\0025a0   "}.lst-kix_6eeexwh7rz-2>li:before{content:"\0025a0   "}.lst-kix_6eeexwh7rz-6>li:before{content:"\0025cf   "}.lst-kix_6eeexwh7rz-1>li:before{content:"\0025cb   "}.lst-kix_6eeexwh7rz-5>li:before{content:"\0025a0   "}.lst-kix_quzgpc4u83ik-5>li:before{content:"\0025a0   "}.lst-kix_le5rw7vfsnpe-0>li:before{content:"\0025cf   "}.lst-kix_quzgpc4u83ik-6>li:before{content:"\0025cf   "}.lst-kix_6eeexwh7rz-3>li:before{content:"\0025cf   "}.lst-kix_quzgpc4u83ik-7>li:before{content:"\0025cb   "}.lst-kix_le5rw7vfsnpe-4>li:before{content:"\0025cb   "}.lst-kix_quzgpc4u83ik-0>li:before{content:"\0025cf   "}.lst-kix_le5rw7vfsnpe-1>li:before{content:"\0025cb   "}.lst-kix_le5rw7vfsnpe-5>li:before{content:"\0025a0   "}.lst-kix_quzgpc4u83ik-1>li:before{content:"\0025cb   "}.lst-kix_6eeexwh7rz-0>li:before{content:"\0025cf   "}.lst-kix_quzgpc4u83ik-4>li:before{content:"\0025cb   "}.lst-kix_le5rw7vfsnpe-2>li:before{content:"\0025a0   "}.lst-kix_quzgpc4u83ik-2>li:before{content:"\0025a0   "}.lst-kix_le5rw7vfsnpe-3>li:before{content:"\0025cf   "}.lst-kix_quzgpc4u83ik-3>li:before{content:"\0025cf   "}.lst-kix_1ul70wjdpz75-3>li:before{content:"\0025cf   "}.lst-kix_1ul70wjdpz75-2>li:before{content:"\0025a0   "}.lst-kix_1ul70wjdpz75-4>li:before{content:"\0025cb   "}.lst-kix_1ul70wjdpz75-0>li:before{content:"\0025cf   "}.lst-kix_1ul70wjdpz75-8>li:before{content:"\0025a0   "}.lst-kix_1ul70wjdpz75-1>li:before{content:"\0025cb   "}ul.lst-kix_bnqoz09lekie-8{list-style-type:none}ul.lst-kix_bnqoz09lekie-7{list-style-type:none}ul.lst-kix_bnqoz09lekie-6{list-style-type:none}ul.lst-kix_bnqoz09lekie-5{list-style-type:none}ul.lst-kix_bnqoz09lekie-4{list-style-type:none}.lst-kix_1ul70wjdpz75-7>li:before{content:"\0025cb   "}ul.lst-kix_bnqoz09lekie-3{list-style-type:none}ul.lst-kix_bnqoz09lekie-2{list-style-type:none}.lst-kix_1ul70wjdpz75-6>li:before{content:"\0025cf   "}ul.lst-kix_bnqoz09lekie-1{list-style-type:none}ul.lst-kix_bnqoz09lekie-0{list-style-type:none}.lst-kix_1ul70wjdpz75-5>li:before{content:"\0025a0   "}ul.lst-kix_le5rw7vfsnpe-8{list-style-type:none}ul.lst-kix_le5rw7vfsnpe-7{list-style-type:none}ul.lst-kix_le5rw7vfsnpe-6{list-style-type:none}ul.lst-kix_le5rw7vfsnpe-5{list-style-type:none}ul.lst-kix_le5rw7vfsnpe-4{list-style-type:none}ul.lst-kix_le5rw7vfsnpe-3{list-style-type:none}ul.lst-kix_le5rw7vfsnpe-2{list-style-type:none}ul.lst-kix_le5rw7vfsnpe-1{list-style-type:none}ul.lst-kix_le5rw7vfsnpe-0{list-style-type:none}.lst-kix_bnqoz09lekie-8>li:before{content:"\0025a0   "}.lst-kix_ph3vtg5kepvs-4>li:before{content:"\0025cb   "}.lst-kix_ph3vtg5kepvs-5>li:before{content:"\0025a0   "}.lst-kix_ph3vtg5kepvs-6>li:before{content:"\0025cf   "}.lst-kix_ph3vtg5kepvs-7>li:before{content:"\0025cb   "}.lst-kix_ph3vtg5kepvs-8>li:before{content:"\0025a0   "}.lst-kix_knetqcmjln5p-3>li:before{content:"\0025cf   "}ul.lst-kix_knetqcmjln5p-4{list-style-type:none}ul.lst-kix_knetqcmjln5p-3{list-style-type:none}ul.lst-kix_knetqcmjln5p-6{list-style-type:none}ul.lst-kix_knetqcmjln5p-5{list-style-type:none}.lst-kix_knetqcmjln5p-1>li:before{content:"\0025cb   "}.lst-kix_knetqcmjln5p-5>li:before{content:"\0025a0   "}ul.lst-kix_knetqcmjln5p-8{list-style-type:none}ul.lst-kix_knetqcmjln5p-7{list-style-type:none}.lst-kix_knetqcmjln5p-0>li:before{content:"\0025cf   "}.lst-kix_knetqcmjln5p-4>li:before{content:"\0025cb   "}.lst-kix_knetqcmjln5p-7>li:before{content:"\0025cb   "}.lst-kix_knetqcmjln5p-6>li:before{content:"\0025cf   "}.lst-kix_knetqcmjln5p-2>li:before{content:"\0025a0   "}ul.lst-kix_1ul70wjdpz75-3{list-style-type:none}ul.lst-kix_1ul70wjdpz75-4{list-style-type:none}ul.lst-kix_1ul70wjdpz75-1{list-style-type:none}ul.lst-kix_1ul70wjdpz75-2{list-style-type:none}ul.lst-kix_fkxgfa7jza0l-8{list-style-type:none}ul.lst-kix_1ul70wjdpz75-7{list-style-type:none}ul.lst-kix_fkxgfa7jza0l-7{list-style-type:none}ul.lst-kix_1ul70wjdpz75-8{list-style-type:none}ul.lst-kix_fkxgfa7jza0l-6{list-style-type:none}ul.lst-kix_1ul70wjdpz75-5{list-style-type:none}ul.lst-kix_fkxgfa7jza0l-5{list-style-type:none}ul.lst-kix_1ul70wjdpz75-6{list-style-type:none}ul.lst-kix_fkxgfa7jza0l-4{list-style-type:none}ul.lst-kix_fkxgfa7jza0l-3{list-style-type:none}ul.lst-kix_fkxgfa7jza0l-2{list-style-type:none}ul.lst-kix_fkxgfa7jza0l-1{list-style-type:none}ul.lst-kix_fkxgfa7jza0l-0{list-style-type:none}ul.lst-kix_1ul70wjdpz75-0{list-style-type:none}.lst-kix_knetqcmjln5p-8>li:before{content:"\0025a0   "}.lst-kix_fkxgfa7jza0l-2>li:before{content:"\0025a0   "}.lst-kix_fkxgfa7jza0l-3>li:before{content:"\0025cf   "}ul.lst-kix_ph3vtg5kepvs-6{list-style-type:none}.lst-kix_fkxgfa7jza0l-0>li:before{content:"\0025cf   "}.lst-kix_fkxgfa7jza0l-1>li:before{content:"\0025cb   "}.lst-kix_fkxgfa7jza0l-4>li:before{content:"\0025cb   "}.lst-kix_fkxgfa7jza0l-5>li:before{content:"\0025a0   "}ul.lst-kix_ph3vtg5kepvs-5{list-style-type:none}ul.lst-kix_ph3vtg5kepvs-8{list-style-type:none}ul.lst-kix_ph3vtg5kepvs-7{list-style-type:none}ul.lst-kix_6eeexwh7rz-5{list-style-type:none}ul.lst-kix_6eeexwh7rz-4{list-style-type:none}ul.lst-kix_6eeexwh7rz-7{list-style-type:none}ul.lst-kix_6eeexwh7rz-6{list-style-type:none}ul.lst-kix_6eeexwh7rz-8{list-style-type:none}.lst-kix_6eeexwh7rz-8>li:before{content:"\0025a0   "}ul.lst-kix_ph3vtg5kepvs-2{list-style-type:none}ul.lst-kix_ph3vtg5kepvs-1{list-style-type:none}ul.lst-kix_ph3vtg5kepvs-4{list-style-type:none}ul.lst-kix_ph3vtg5kepvs-3{list-style-type:none}ul.lst-kix_6eeexwh7rz-1{list-style-type:none}ul.lst-kix_6eeexwh7rz-0{list-style-type:none}ul.lst-kix_6eeexwh7rz-3{list-style-type:none}.lst-kix_6eeexwh7rz-7>li:before{content:"\0025cb   "}ul.lst-kix_ph3vtg5kepvs-0{list-style-type:none}ul.lst-kix_6eeexwh7rz-2{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_fkxgfa7jza0l-8>li:before{content:"\0025a0   "}.lst-kix_fkxgfa7jza0l-6>li:before{content:"\0025cf   "}.lst-kix_fkxgfa7jza0l-7>li:before{content:"\0025cb   "}ul.lst-kix_knetqcmjln5p-0{list-style-type:none}ul.lst-kix_knetqcmjln5p-2{list-style-type:none}ul.lst-kix_knetqcmjln5p-1{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c24{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:107.5pt;border-top-color:#000000;border-bottom-style:solid}.c31{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:64pt;border-top-color:#000000;border-bottom-style:solid}.c11{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c26{-webkit-text-decoration-skip:none;color:#000000;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:13pt;font-family:"Arial";font-style:normal}.c14{-webkit-text-decoration-skip:none;color:#000000;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c0{color:#660000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center;height:11pt}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Arial";font-style:normal}.c37{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c16{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c17{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c18{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c19{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c30{margin-left:54pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-indent:18pt;text-align:left}.c34{padding-top:14pt;padding-bottom:4pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c12{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c29{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c3{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c38{padding-top:14pt;padding-bottom:6pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c35{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c28{color:#000000;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c36{color:#000000;text-decoration:none;vertical-align:baseline;font-size:25pt;font-family:"Arial";font-style:normal}.c41{color:#000000;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Comfortaa";font-style:normal}.c42{font-weight:400;vertical-align:baseline;font-size:13pt;font-family:"Arial";font-style:normal}.c5{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-indent:36pt;text-align:left}.c39{margin-left:36pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c21{margin-left:36pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c43{font-weight:400;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c13{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c25{border-spacing:0;border-collapse:collapse;margin-right:auto}.c22{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c40{padding:0;margin:0}.c6{color:inherit;text-decoration:inherit}.c20{margin-left:36pt;padding-left:0pt}.c44{color:#000000;text-decoration:none}.c32{height:25pt}.c27{font-weight:700}.c8{height:11pt}.c33{font-style:italic}.c23{font-size:12pt}.c45{font-size:13pt}.c46{page-break-after:avoid}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c22 doc-content"><h3 class="c34" id="h.cc3vfysa7vrt"><span class="c27 c41">DESTILACI&Oacute;N DE IA</span></h3><p class="c2 c8"><span class="c4"></span></p><p class="c2 c8"><span class="c4"></span></p><p class="c15"><span class="c26">Hecho por Izan Morcillo Mart&iacute;n</span></p><p class="c15 c8"><span class="c14"></span></p><p class="c2 c8"><span class="c14"></span></p><p class="c2 c8"><span class="c14"></span></p><p class="c15 c8"><span class="c14"></span></p><p class="c8 c15"><span class="c14"></span></p><p class="c15 c8"><span class="c14"></span></p><p class="c29"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 629.95px; height: 253.38px;"><img alt="" src="images/image4.png" style="width: 629.95px; height: 253.38px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><p class="c7"><span class="c14"></span></p><h3 class="c34" id="h.rgoxnqtii5lo"><span class="c27 c36">&Iacute;ndice</span></h3><p class="c2 c8"><span class="c4"></span></p><p class="c2"><span class="c45">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c2"><span class="c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c21"><span class="c13 c43"><a class="c6" href="#h.cc3vfysa7vrt">DESTILACI&Oacute;N DE IA</a></span></p><p class="c21 c8"><span class="c11"></span></p><p class="c21"><span class="c13 c42"><a class="c6" href="#h.rgoxnqtii5lo">&Iacute;ndice</a></span></p><p class="c5"><span class="c11"><a class="c6" href="#h.p9nth7t12tpv">Introducci&oacute;n</a></span></p><p class="c5 c8"><span class="c11"></span></p><p class="c5"><span class="c11"><a class="c6" href="#h.fkdc0e6n5pfd">Qu&eacute; es la destilaci&oacute;n de modelos?</a></span></p><p class="c5 c8"><span class="c11"></span></p><p class="c5"><span class="c11"><a class="c6" href="#h.n6aburn0nkp">Qu&eacute; modelos us&eacute;?</a></span></p><p class="c5 c8"><span class="c11"></span></p><p class="c5"><span class="c11"><a class="c6" href="#h.2kqqlsxbxblt">Como entren&eacute; el modelo destilado</a></span></p><p class="c30"><span class="c11"><a class="c6" href="#h.ggh0aa3d2v9g">Pasos</a></span></p><p class="c30 c8"><span class="c11"></span></p><p class="c5"><span class="c11"><a class="c6" href="#h.ex6o984q46n7">Diferencias entre el profesor y el estudiante destilado</a></span></p><p class="c5 c8"><span class="c11"></span></p><p class="c5"><span class="c11"><a class="c6" href="#h.x0k4i2a1pbtl">Por qu&eacute; el modelo destilado es m&aacute;s tonto?</a></span></p><p class="c5 c8"><span class="c11"></span></p><p class="c5"><span class="c11"><a class="c6" href="#h.w02ulsv5ygyx">Como cargar&iacute;amos mi modelo destilado en LM Studio?</a></span></p><p class="c30"><span class="c11"><a class="c6" href="#h.l5h7jgldd6dy">Convertir el modelo HuggingFace &rarr; GGUF</a></span></p><p class="c30 c8"><span class="c11"></span></p><p class="c39"><span class="c11"><a class="c6" href="#h.5a9qoms0fwuw">Medir que tan listo es el estudiante en comparaci&oacute;n con el maestro mediante divergencia KL</a></span></p><p class="c39 c8"><span class="c11"></span></p><p class="c5"><span class="c11"><a class="c6" href="#h.4m19kyjsqzg">Pila de acertijos que us&eacute;</a></span></p><p class="c5 c8"><span class="c11"></span></p><p class="c5"><span class="c11"><a class="c6" href="#h.4qzh7x65k7k">Gr&aacute;ficas</a></span></p><p class="c30"><span class="c11"><a class="c6" href="#h.3a6yl55lxgqf">Medici&oacute;n de la calidad de respuestas</a></span></p><p class="c30"><span class="c11"><a class="c6" href="#h.d6aqil68l7vb">Medici&oacute;n de la velocidad de respuestas</a></span></p><p class="c30 c8"><span class="c11"></span></p><p class="c5"><span class="c11"><a class="c6" href="#h.8i6ud2hidx5e">Conclusi&oacute;n</a></span></p><p class="c2 c8"><span class="c43 c44"></span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2 c8"><span class="c4"></span></p><h1 class="c38" id="h.p9nth7t12tpv"><span class="c28 c27">Introducci&oacute;n</span></h1><p class="c2 c8"><span class="c4"></span></p><p class="c3"><span class="c23">En los &uacute;ltimos a&ntilde;os, los modelos de lenguaje de gran tama&ntilde;o han demostrado un rendimiento sobresaliente en tareas de comprensi&oacute;n, generaci&oacute;n y razonamiento. Sin embargo, su elevado coste computacional dificulta su uso en entornos con recursos limitados o en aplicaciones que requieren respuestas r&aacute;pidas. En este contexto surge la destilaci&oacute;n de modelos (</span><span class="c33 c23">knowledge distillation</span><span class="c1">), una t&eacute;cnica que permite transferir el comportamiento de un modelo grande (teacher) a uno m&aacute;s peque&ntilde;o (student), reduciendo su tama&ntilde;o y acelerando su ejecuci&oacute;n sin necesidad de reentrenar desde cero.</span></p><p class="c3"><span class="c23">El objetivo de este trabajo es analizar de forma pr&aacute;ctica el proceso de destilaci&oacute;n y evaluar su impacto tanto en la precisi&oacute;n como en la velocidad de generaci&oacute;n. Para ello se han comparado dos modelos: un modelo base de mayor capacidad (GPT&#8209;Medium) y un modelo reducido obtenido mediante destilaci&oacute;n (GPT&#8209;Small). La evaluaci&oacute;n se ha realizado mediante un conjunto de preguntas t&eacute;cnicas relacionadas con el aprendizaje autom&aacute;tico, as&iacute; como mediante la medici&oacute;n de </span><span class="c23 c33">tokens por segundo</span><span class="c1">&nbsp;como indicador de eficiencia.</span></p><p class="c3"><span class="c1">A lo largo del proyecto se han generado gr&aacute;ficas comparativas que permiten visualizar de manera clara las diferencias entre ambos modelos. Estas representaciones facilitan la interpretaci&oacute;n de los resultados y permiten valorar hasta qu&eacute; punto la destilaci&oacute;n consigue un equilibrio adecuado entre rendimiento y eficiencia. El trabajo concluye con un an&aacute;lisis cr&iacute;tico de los resultados obtenidos y una reflexi&oacute;n sobre la utilidad real de la destilaci&oacute;n en escenarios pr&aacute;cticos.</span></p><p class="c3 c8"><span class="c1"></span></p><h1 class="c38" id="h.fkdc0e6n5pfd"><span class="c27 c28">Qu&eacute; es la destilaci&oacute;n de modelos?</span></h1><p class="c2 c8"><span class="c4"></span></p><p class="c2"><span class="c1">La destilaci&oacute;n de modelos (knowledge distillation) es una t&eacute;cnica utilizada en aprendizaje autom&aacute;tico para transferir el conocimiento de un modelo grande y preciso (teacher) a un modelo m&aacute;s peque&ntilde;o y eficiente (student).</span></p><p class="c2"><span class="c1">El proceso consiste en entrenar el modelo peque&ntilde;o para que imite las salidas del modelo grande, no solo las etiquetas reales. De esta forma, el student aprende patrones, relaciones y comportamientos que el teacher ya ha capturado, pero con un coste computacional mucho menor.</span></p><p class="c2"><span class="c1">La destilaci&oacute;n permite:</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">-reducir el tama&ntilde;o del modelo,</span></p><p class="c2"><span class="c1">-acelerar la generaci&oacute;n de texto,</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">-disminuir el consumo de memoria,</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">-mantener un rendimiento razonablemente cercano al modelo original.</span></p><p class="c2"><span class="c1">Es una t&eacute;cnica muy utilizada cuando se necesita desplegar modelos en dispositivos con recursos limitados o cuando la velocidad es un factor cr&iacute;tico.</span></p><p class="c2 c8"><span class="c1"></span></p><h1 class="c19" id="h.n6aburn0nkp"><span class="c28 c27">Qu&eacute; modelos us&eacute;?</span></h1><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">En este trabajo se utilizaron dos modelos de lenguaje con distinta capacidad y prop&oacute;sito:</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">-GPT&#8209;Medium (Teacher)</span></p><p class="c2"><span class="c1">Es el modelo de mayor tama&ntilde;o y precisi&oacute;n. Se utiliza como referencia porque ofrece respuestas m&aacute;s completas y coherentes. Su funci&oacute;n en el proyecto es actuar como modelo maestro del cual se extrae el conocimiento.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">Caracter&iacute;sticas clave:</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">mayor n&uacute;mero de par&aacute;metros,</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">mayor precisi&oacute;n en tareas t&eacute;cnicas,</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">menor velocidad de generaci&oacute;n.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">-GPT&#8209;Small (Student destilado)</span></p><p class="c2"><span class="c1">Es el modelo reducido obtenido mediante destilaci&oacute;n. Su objetivo es imitar el comportamiento del teacher, pero con un coste computacional mucho menor.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">Caracter&iacute;sticas clave:</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">menos par&aacute;metros,</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">menor precisi&oacute;n,</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">mucha mayor velocidad de generaci&oacute;n,</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">ideal para entornos donde la eficiencia es prioritaria.</span></p><p class="c2 c8"><span class="c1"></span></p><h1 class="c19" id="h.2kqqlsxbxblt"><span class="c28 c27">Como entren&eacute; el modelo destilado</span></h1><h2 class="c17" id="h.ggh0aa3d2v9g"><span class="c9">Pasos</span></h2><p class="c3"><span class="c4">Empezaremos instalando las librerias habiendo descargado previamente Python</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 510.66px; height: 645.50px;"><img alt="" src="images/image15.png" style="width: 510.66px; height: 645.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c4">Ya las tendr&iacute;amos correctamente descargadas y aprovech&eacute; para actualizar pip </span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 679.27px; height: 203.10px;"><img alt="" src="images/image11.png" style="width: 679.27px; height: 203.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span>Crear&eacute; el archivo de destilacion </span><span class="c13"><a class="c6" href="https://www.google.com/url?q=http://destilacion.py&amp;sa=D&amp;source=editors&amp;ust=1770567517487462&amp;usg=AOvVaw2a1UxXJQNQNehY8xusF4H4">destilacion.py</a></span><span class="c4">&nbsp;y este va a ser mi documento a ejecutar</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 466.00px; height: 814.00px;"><img alt="" src="images/image2.png" style="width: 466.00px; height: 814.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 530.00px; height: 735.00px;"><img alt="" src="images/image6.png" style="width: 530.00px; height: 735.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c8"><span class="c4"></span></p><p class="c3"><span class="c4">Ahora ejecuto el archivo para entrenar el modelo destilado</span></p><p class="c3"><span class="c4">Estuve tiempo realizando la ejecuci&oacute;n y result&oacute; que esta versi&oacute;n de python tan nueva genera problemas as&iacute; que instal&eacute; una funcional.</span></p><p class="c3"><span class="c4">Y hubo varios problemas pero la versi&oacute;n final y FUNCIONAL es esta:</span></p><p class="c3"><span>Version final de </span><span class="c13"><a class="c6" href="https://www.google.com/url?q=http://destilar.py&amp;sa=D&amp;source=editors&amp;ust=1770567517488308&amp;usg=AOvVaw2bkphqmmcDA_PbFNxZo9Rx">DESTILAR.py</a></span></p><p class="c3 c8"><span class="c4"></span></p><p class="c3"><span class="c0">from transformers import AutoModelForCausalLM, AutoTokenizer</span></p><p class="c3"><span class="c0">from datasets import load_dataset</span></p><p class="c3"><span class="c0">import torch</span></p><p class="c3"><span class="c0">import torch.nn.functional as F</span></p><p class="c3"><span class="c0">from transformers import Trainer, TrainingArguments</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0"># 1. Cargar modelos</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0">teacher = AutoModelForCausalLM.from_pretrained(&quot;gpt2-medium&quot;)</span></p><p class="c3"><span class="c0">student = AutoModelForCausalLM.from_pretrained(&quot;gpt2&quot;)</span></p><p class="c3"><span class="c0">tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)</span></p><p class="c3"><span class="c0">tokenizer.pad_token = tokenizer.eos_token &nbsp;# IMPORTANTE</span></p><p class="c3 c8"><span class="c0"></span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0"># 2. Cargar dataset limpio (AG News)</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0">dataset = load_dataset(&quot;ag_news&quot;)</span></p><p class="c3"><span class="c0"># Reducir dataset a 5000 ejemplos (MUY IMPORTANTE PARA CPU)</span></p><p class="c3"><span class="c0">dataset[&quot;train&quot;] = dataset[&quot;train&quot;].select(range(5000))</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0"># 3. Preparar texto</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0">def preparar(batch):</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; texto = batch[&quot;text&quot;]</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; if not isinstance(texto, str):</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; texto = str(texto)</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; texto = texto.strip()</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; if texto == &quot;&quot;:</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; texto = &quot; &quot;</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; return {&quot;text&quot;: texto}</span></p><p class="c3 c8"><span class="c0"></span></p><p class="c3"><span class="c0">dataset = dataset.map(preparar)</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0"># 4. Tokenizar dataset</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0">def tokenize(batch):</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; tokens = tokenizer(</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; batch[&quot;text&quot;],</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; truncation=True,</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; padding=&quot;max_length&quot;,</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; max_length=128</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; )</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; if &quot;labels&quot; in tokens:</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; del tokens[&quot;labels&quot;]</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; return tokens</span></p><p class="c3 c8"><span class="c0"></span></p><p class="c3"><span class="c0">tokenized = dataset.map(tokenize, batched=True)</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0"># 5. Trainer personalizado para distilaci&oacute;n</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0">class DistillationTrainer(Trainer):</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; def compute_loss(self, model, inputs, return_outputs=False, **kwargs):</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; if &quot;labels&quot; in inputs:</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; del inputs[&quot;labels&quot;]</span></p><p class="c3 c8"><span class="c0"></span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; outputs_student = model(**inputs)</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; logits_student = outputs_student.logits</span></p><p class="c3 c8"><span class="c0"></span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; with torch.no_grad():</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; outputs_teacher = teacher(**inputs)</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logits_teacher = outputs_teacher.logits</span></p><p class="c3 c8"><span class="c0"></span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; loss = F.kl_div(</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logits_student.log_softmax(dim=-1),</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logits_teacher.softmax(dim=-1),</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; reduction=&quot;batchmean&quot;</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; )</span></p><p class="c3 c8"><span class="c0"></span></p><p class="c3"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp; return (loss, outputs_student) if return_outputs else loss</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0"># 6. Configuraci&oacute;n del entrenamiento</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0">training_args = TrainingArguments(</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; output_dir=&quot;./destilado&quot;,</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; per_device_train_batch_size=2,</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; num_train_epochs=1,</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; logging_steps=50,</span></p><p class="c3"><span class="c0">)</span></p><p class="c3"><span class="c0">trainer = DistillationTrainer(</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; model=student,</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; args=training_args,</span></p><p class="c3"><span class="c0">&nbsp; &nbsp; train_dataset=tokenized[&quot;train&quot;],</span></p><p class="c3"><span class="c0">)</span></p><p class="c3 c8"><span class="c0"></span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0"># 7. Entrenar</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0">trainer.train()</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0"># 8. Guardar modelo destilado</span></p><p class="c3"><span class="c0"># -----------------------------</span></p><p class="c3"><span class="c0">student.save_pretrained(&quot;mi_modelo_destilado&quot;)</span></p><p class="c3"><span class="c0">tokenizer.save_pretrained(&quot;mi_modelo_destilado&quot;)</span></p><p class="c3 c8"><span class="c4"></span></p><p class="c3"><span class="c4">Y la ejecutamos y empezamos el proceso de entrenamiento de la IA destilada, esta adquiriendo las habilidades de su maestro</span></p><p class="c3 c8"><span class="c4"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 214.67px;"><img alt="" src="images/image8.png" style="width: 601.70px; height: 214.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c8"><span class="c4"></span></p><p class="c3"><span class="c4">Me he visto obligado a bajar los ejemplo con los que la estoy entrenando ya que con 120000 ejemplos iba a tardar una barbaridad y lo baje a 5000</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 330.00px; height: 155.00px;"><img alt="" src="images/image16.png" style="width: 330.00px; height: 155.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c4">Ya termin&oacute; el entrenamiento</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 368.00px;"><img alt="" src="images/image10.png" style="width: 601.70px; height: 368.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c8"><span class="c4"></span></p><p class="c3"><span class="c4">Que deber&iacute;amos de empezar a ver cuando los comparemos:</span></p><ul class="c40 lst-kix_quzgpc4u83ik-0 start"><li class="c3 c20 li-bullet-0"><span>El teacher (GPT&#8209;2 medium) tiene </span><span class="c35 c27">355M par&aacute;metros</span></li><li class="c3 c20 li-bullet-0"><span>El student (GPT&#8209;2 small) tiene </span><span class="c27 c35">124M par&aacute;metros</span></li><li class="c3 c20 li-bullet-0"><span>Aunque entrenemos con 5.000 ejemplos, el student </span><span class="c27">nunca</span><span class="c4">&nbsp;alcanzar&aacute; al teacher</span></li><li class="c3 c20 li-bullet-0"><span class="c4">La KL divergence solo hace que el student &ldquo;imite&rdquo; un poco al teacher</span></li><li class="c3 c20 li-bullet-0"><span class="c4">Pero sigue siendo un modelo m&aacute;s peque&ntilde;o, con menos capacidad</span></li></ul><p class="c3"><span class="c4">As&iacute; que cuando comparemos:</span></p><ul class="c40 lst-kix_knetqcmjln5p-0 start"><li class="c3 c20 li-bullet-0"><span class="c4">Teacher responde mejor</span></li><li class="c3 c20 li-bullet-0"><span class="c4">Student destilado responde m&aacute;s simple, m&aacute;s corto, m&aacute;s limitado y se le notar&aacute; mucho m&aacute;s tonto</span></li></ul><p class="c3 c8"><span class="c4"></span></p><h1 class="c19" id="h.ex6o984q46n7"><span class="c28 c27">Diferencias entre el profesor y el estudiante destilado</span></h1><p class="c2 c8"><span class="c4"></span></p><p class="c2"><span class="c1">La diferencia fundamental entre ambos modelos est&aacute; en su capacidad, tama&ntilde;o y objetivo dentro del proceso de destilaci&oacute;n. Aunque comparten arquitectura, no juegan el mismo papel.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">Modelo Teacher (GPT&#8209;Medium)</span></p><p class="c2"><span class="c1">Tiene m&aacute;s par&aacute;metros y, por tanto, mayor capacidad de representaci&oacute;n.</span></p><p class="c2"><span class="c1">Produce respuestas m&aacute;s precisas, completas y coherentes.</span></p><p class="c2"><span class="c1">Requiere m&aacute;s memoria, m&aacute;s tiempo de inferencia y m&aacute;s recursos.</span></p><p class="c2"><span class="c1">Act&uacute;a como fuente de conocimiento durante la destilaci&oacute;n.</span></p><p class="c2"><span class="c1">Su rendimiento es el est&aacute;ndar contra el que se eval&uacute;a al modelo peque&ntilde;o.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">Modelo Student (GPT&#8209;Small destilado)</span></p><p class="c2"><span class="c1">Tiene menos par&aacute;metros, lo que reduce su complejidad.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">Es mucho m&aacute;s r&aacute;pido generando texto (m&aacute;s tokens por segundo).</span></p><p class="c2"><span class="c1">Consume menos memoria y es m&aacute;s f&aacute;cil de desplegar.</span></p><p class="c2"><span class="c1">Su precisi&oacute;n es inferior, porque no puede capturar todos los patrones del teacher.</span></p><p class="c2"><span class="c1">Su objetivo no es igualar al teacher, sino aproximarlo con mucha m&aacute;s eficiencia.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">En resumen:</span></p><p class="c2"><span class="c1">el teacher es m&aacute;s inteligente pero m&aacute;s lento; el student es m&aacute;s r&aacute;pido pero menos preciso. &nbsp;</span></p><p class="c2"><span class="c1">Ese equilibrio es exactamente lo que se busca en destilaci&oacute;n.</span></p><p class="c2 c8"><span class="c4"></span></p><h1 class="c19" id="h.x0k4i2a1pbtl"><span class="c28 c27">Por qu&eacute; el modelo destilado es m&aacute;s tonto?</span></h1><p class="c2 c8"><span class="c4"></span></p><p class="c2"><span class="c1">Esto ocurre por varias razones:</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">1. Menos par&aacute;metros = menos capacidad de aprendizaje</span></p><p class="c2"><span class="c1">Un modelo grande puede almacenar patrones, relaciones y matices que uno peque&ntilde;o simplemente no puede representar.</span></p><p class="c2"><span class="c1">Es como comparar:</span></p><p class="c2"><span class="c1">un libro de 500 p&aacute;ginas (teacher)</span></p><p class="c2"><span class="c1">con un resumen de 80 p&aacute;ginas (student)</span></p><p class="c2"><span class="c1">El resumen es &uacute;til, pero inevitablemente pierde detalle.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">2. La destilaci&oacute;n transmite comportamiento, no inteligencia completa</span></p><p class="c2"><span class="c1">El student aprende a imitar las respuestas del teacher, pero:</span></p><p class="c2"><span class="c1">no aprende todos los razonamientos internos,</span></p><p class="c2"><span class="c1">no aprende todos los matices,</span></p><p class="c2"><span class="c1">no aprende todos los patrones estad&iacute;sticos.</span></p><p class="c2"><span class="c1">Solo aprende lo que el teacher expresa en sus salidas.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">3. El modelo peque&ntilde;o generaliza peor</span></p><p class="c2"><span class="c1">Con menos capacidad, tiende a:</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">simplificar demasiado,</span></p><p class="c2"><span class="c1">cometer errores conceptuales,</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">dar respuestas incompletas,</span></p><p class="c2"><span class="c1">confundirse en preguntas t&eacute;cnicas.</span></p><p class="c2"><span class="c1">Esto es normal y esperado.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">4. La destilaci&oacute;n prioriza velocidad sobre precisi&oacute;n</span></p><p class="c2"><span class="c1">El objetivo del student no es ser perfecto, sino ser r&aacute;pido y eficiente.</span></p><p class="c2"><span class="c1">Por eso sacrifica parte de la calidad.</span></p><p class="c2 c8"><span class="c1"></span></p><h1 class="c19" id="h.w02ulsv5ygyx"><span class="c28 c27">Como cargar&iacute;amos mi modelo destilado en LM Studio?</span></h1><h2 class="c17" id="h.l5h7jgldd6dy"><span class="c9">Convertir el modelo HuggingFace &rarr; GGUF</span></h2><p class="c2"><span class="c1">Esto lleva algunos pasos pero yo lo hice de esta manera:</span></p><p class="c2"><span class="c1">Entr&eacute; en el repositorio de github siguiente</span></p><p class="c2"><span class="c13 c23"><a class="c6" href="https://www.google.com/url?q=https://github.com/ggml-org/llama.cpp?utm_source%3Dcopilot.com&amp;sa=D&amp;source=editors&amp;ust=1770567517503692&amp;usg=AOvVaw2Ie3CSCpZPThOAYSaOdTGR">https://github.com/ggml-org/llama.cpp?utm_source=copilot.com</a></span><span class="c1">&nbsp;y aqu&iacute; descargu&eacute; el zip y me enfoqu&eacute; en el .py que dice convert_hf_to_GGUF o algo as&iacute;.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 404.00px;"><img alt="" src="images/image13.png" style="width: 601.70px; height: 404.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 539.00px; height: 943.00px;"><img alt="" src="images/image9.png" style="width: 539.00px; height: 943.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1">Y en este directorio ejecut&eacute; el comando para convertir mi modelo en GGUF</span></p><p class="c2"><span class="c16">python .\convert_hf_to_gguf.py &quot;C:\Users\Izan\mi_modelo_destilado&quot; --outfile &quot;C:\Users\Izan\mi_modelo_destilado.gguf&quot;</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 695.67px; height: 504.18px;"><img alt="" src="images/image3.png" style="width: 695.67px; height: 504.18px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2"><span class="c4">Y ya lo tendr&iacute;amos destilado en el formato adecuado para meter en LM Studio.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 371.00px; height: 504.00px;"><img alt="" src="images/image7.png" style="width: 371.00px; height: 504.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c8"><span class="c4"></span></p><p class="c8 c12"><span class="c4"></span></p><h1 class="c12 c46" id="h.5a9qoms0fwuw"><span class="c28 c27">Medir que tan listo es el estudiante en comparaci&oacute;n con el maestro mediante divergencia KL</span></h1><p class="c3 c8"><span class="c4"></span></p><ul class="c40 lst-kix_6eeexwh7rz-0 start"><li class="c3 c20 li-bullet-0"><span class="c4">Ese script calcula el % de similitud entre el modelo destilado y el maestro</span></li></ul><p class="c3"><span class="c4">&rarr; esto demuestra que el modelo peque&ntilde;o se parece, pero no es igual.</span></p><p class="c3"><span class="c4">Lo que podemos medir:</span></p><ul class="c40 lst-kix_fkxgfa7jza0l-0 start"><li class="c3 c20 li-bullet-0"><span class="c4">qu&eacute; tan diferente es la distribuci&oacute;n de probabilidades del student respecto al teacher</span></li><li class="c3 c20 li-bullet-0"><span class="c4">qu&eacute; tan bien imita el student al teacher</span></li><li class="c3 c20 li-bullet-0"><span class="c4">qu&eacute; tan bien predice el siguiente token</span></li></ul><p class="c3"><span class="c4">Medimos la distancia entre el modelo grande y el modelo destilado usando KL Divergence.</span></p><p class="c3"><span class="c4">Cuanto mayor es la KL, m&aacute;s diferente es el comportamiento del student respecto al teacher.</span></p><p class="c3"><span class="c4">Tambi&eacute;n medimos la Perplexity, que indica qu&eacute; tan bien predice el modelo el siguiente token.</span></p><p class="c3"><span class="c4">El modelo destilado tiene mayor Perplexity y mayor KL, lo que demuestra que es m&aacute;s limitado, pero tambi&eacute;n m&aacute;s ligero y eficiente.</span></p><p class="c3"><span class="c26">Este es el script que mide la divergencia KL:</span></p><p class="c3"><span class="c4">import torch import torch.nn.functional as F from transformers import AutoModelForCausalLM, AutoTokenizer from datasets import load_dataset # ----------------------------- # 1. Cargar modelos # ----------------------------- teacher = AutoModelForCausalLM.from_pretrained(&quot;gpt2-medium&quot;) student = AutoModelForCausalLM.from_pretrained(&quot;mi_modelo_destilado&quot;) tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;) tokenizer.pad_token = tokenizer.eos_token # ----------------------------- # 2. Cargar dataset peque&ntilde;o # ----------------------------- dataset = load_dataset(&quot;ag_news&quot;) subset = dataset[&quot;test&quot;].select(range(200)) # 200 ejemplos para medir # ----------------------------- # 3. Funci&oacute;n para calcular KL # ----------------------------- def calcular_kl(texto): inputs = tokenizer(texto, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=128) with torch.no_grad(): logits_teacher = teacher(**inputs).logits logits_student = student(**inputs).logits kl = F.kl_div( logits_student.log_softmax(dim=-1), logits_teacher.softmax(dim=-1), reduction=&quot;batchmean&quot; ) return kl.item() # ----------------------------- # 4. Calcular KL promedio # ----------------------------- total_kl = 0 for i, ejemplo in enumerate(subset): texto = ejemplo[&quot;text&quot;] total_kl += calcular_kl(texto) kl_promedio = total_kl / len(subset) print(&quot;\n====================================&quot;) print(&quot; KL Divergence promedio:&quot;) print(f&quot; {kl_promedio:.4f}&quot;) print(&quot;====================================\n&quot;)</span></p><p class="c3 c8"><span class="c4"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 353.00px; height: 277.00px;"><img alt="" src="images/image1.png" style="width: 353.00px; height: 277.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span>La KL Divergence entre el modelo destilado y el modelo maestro es de 24.98. Esto indica que el modelo peque&ntilde;o se desv&iacute;a mucho del comportamiento del modelo grande. En otras palabras, el student es significativamente m&aacute;s limitado, lo cual confirma que la destilaci&oacute;n ha comprimido el modelo a costa de perder capacidad.</span></p><p class="c2"><span class="c4">Tambi&eacute;n podemos medir el perplexity o PL creo que tambi&eacute;n es interesante a&ntilde;adirlo en este proyecto ya que el perplexity es una medida que indica qu&eacute; tan bien un modelo de lenguaje predice el siguiente token. &nbsp;</span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2"><span class="c4">Cuanto m&aacute;s baja es la Perplexity, mejor predice el modelo; cuanto m&aacute;s alta, peor lo hace.</span></p><p class="c2 c8"><span class="c4"></span></p><p class="c2"><span class="c4">El modelo destilado obtuvo una Perplexity promedio de 62.1. Esto indica que su capacidad para predecir el siguiente token es moderada, pero claramente inferior al modelo maestro. Combinado con la KL Divergence de 24.98, queda demostrado que el student es m&aacute;s limitado, lo cual es coherente con su menor tama&ntilde;o y con el proceso de destilaci&oacute;n.</span></p><p class="c2 c8"><span class="c4"></span></p><table class="c25"><tr class="c32"><td class="c31" colspan="1" rowspan="1"><p class="c29"><span class="c27">Perplexity</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c29"><span class="c27">Interpretaci&oacute;n</span></p></td></tr><tr class="c32"><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c4">20&ndash;40</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c2"><span class="c4">Modelo competente</span></p></td></tr><tr class="c32"><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c4">40&ndash;80</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c2"><span class="c4">Modelo limitado</span></p></td></tr><tr class="c32"><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c4">80&ndash;150</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c2"><span class="c4">Modelo flojo</span></p></td></tr><tr class="c32"><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c4">&gt;150</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c2 c8"><span class="c4"></span></p></td></tr></table><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 395.00px; height: 168.00px;"><img alt="" src="images/image5.png" style="width: 395.00px; height: 168.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c8"><span class="c4"></span></p><h1 class="c37" id="h.4m19kyjsqzg"><span class="c18">Pila de acertijos que us&eacute;</span></h1><p class="c2 c8"><span class="c4"></span></p><p class="c2"><span class="c16">1. &iquest;Qu&eacute; es la entrop&iacute;a cruzada (cross&#8209;entropy) en machine learning?</span></p><p class="c2"><span class="c1">Es una funci&oacute;n de p&eacute;rdida que mide la diferencia entre la distribuci&oacute;n real de las etiquetas y la distribuci&oacute;n predicha por el modelo. Se usa mucho en clasificaci&oacute;n porque penaliza fuertemente las predicciones incorrectas con alta confianza.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c16">2. &iquest;Qu&eacute; diferencia principal existe entre un modelo autoregresivo y uno enmascarado (masked language model)?</span></p><p class="c2"><span class="c1">Un modelo autoregresivo predice la siguiente palabra bas&aacute;ndose en las anteriores.</span></p><p class="c2"><span class="c1">Un modelo enmascarado predice palabras ocultas dentro de una frase completa, no solo la siguiente.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c16">3. &iquest;Qu&eacute; es el gradiente en el contexto del descenso de gradiente?</span></p><p class="c2"><span class="c1">Es el vector de derivadas parciales que indica la direcci&oacute;n en la que la funci&oacute;n de p&eacute;rdida aumenta m&aacute;s r&aacute;pido. El descenso de gradiente ajusta los par&aacute;metros movi&eacute;ndose en la direcci&oacute;n opuesta.</span></p><p class="c2 c8"><span class="c16"></span></p><p class="c2"><span class="c16">4. &iquest;Para qu&eacute; sirve la funci&oacute;n softmax?</span></p><p class="c2"><span class="c1">Convierte un vector de valores en una distribuci&oacute;n de probabilidad, donde todos los valores est&aacute;n entre 0 y 1 y suman 1. Se usa en la capa final de modelos de clasificaci&oacute;n.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c16">5. &iquest;Qu&eacute; es un embedding?</span></p><p class="c2"><span class="c1">Es una representaci&oacute;n num&eacute;rica densa que captura el significado de palabras, frases o elementos, permitiendo que el modelo trabaje con relaciones sem&aacute;nticas.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c16">6. &iquest;Qu&eacute; significa que un modelo est&eacute; sobreajustado (overfitted)?</span></p><p class="c2"><span class="c1">Que ha aprendido demasiado bien los datos de entrenamiento, incluyendo ruido y patrones irrelevantes, y por eso rinde peor en datos nuevos.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c16">7. &iquest;Qu&eacute; es la atenci&oacute;n (attention mechanism) en los Transformers?</span></p><p class="c2"><span class="c1">Es un mecanismo que permite al modelo ponderar qu&eacute; partes de la entrada son m&aacute;s relevantes para generar la salida, asignando pesos a cada token seg&uacute;n su importancia contextual.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c16">8. &iquest;Qu&eacute; ventaja tiene la destilaci&oacute;n de modelos?</span></p><p class="c2"><span class="c1">Permite transferir el conocimiento de un modelo grande a uno m&aacute;s peque&ntilde;o, logrando modelos m&aacute;s r&aacute;pidos y ligeros sin perder demasiada precisi&oacute;n.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c16">9. &iquest;Qu&eacute; es un dataset de validaci&oacute;n?</span></p><p class="c2"><span class="c1">Es un conjunto de datos separado del entrenamiento que se usa para ajustar hiperpar&aacute;metros y evaluar el rendimiento del modelo durante el entrenamiento sin sesgo.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c16">10. &iquest;Qu&eacute; es un hiperpar&aacute;metro?</span></p><p class="c2"><span class="c1">Es un par&aacute;metro que no aprende el modelo directamente, sino que se define antes del entrenamiento, como la tasa de aprendizaje, el n&uacute;mero de capas o el tama&ntilde;o del batch.</span></p><p class="c2 c8"><span class="c1"></span></p><h1 class="c19" id="h.4qzh7x65k7k"><span class="c28 c27">Gr&aacute;ficas</span></h1><h2 class="c17" id="h.3a6yl55lxgqf"><span class="c9">Medici&oacute;n de la calidad de respuestas</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 836.25px; height: 333.24px;"><img alt="" src="images/image14.png" style="width: 836.25px; height: 333.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h2><p class="c2 c8"><span class="c4"></span></p><h2 class="c17" id="h.d6aqil68l7vb"><span class="c9">Medici&oacute;n de la velocidad de respuestas </span></h2><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 401.33px;"><img alt="" src="images/image12.png" style="width: 601.70px; height: 401.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c19" id="h.8i6ud2hidx5e"><span class="c28 c27">Conclusi&oacute;n</span></h1><p class="c2 c8"><span class="c4"></span></p><p class="c2"><span class="c1">A lo largo de este proyecto he desarrollado un proceso completo de destilaci&oacute;n de modelos de lenguaje, evaluando tanto su rendimiento como su eficiencia computacional. El objetivo principal era comprobar si un modelo reducido (GPT&#8209;Small, student) pod&iacute;a aproximarse al comportamiento de un modelo mayor (GPT&#8209;Medium, teacher) manteniendo una mejora significativa en velocidad y consumo de recursos.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">En primer lugar, prepar&eacute; un conjunto de preguntas t&eacute;cnicas dise&ntilde;adas para evaluar comprensi&oacute;n conceptual, capacidad explicativa y coherencia en tareas propias del &aacute;mbito del aprendizaje autom&aacute;tico. Este conjunto permiti&oacute; medir de forma objetiva el rendimiento de ambos modelos. Los resultados mostraron una diferencia clara: el modelo teacher alcanz&oacute; 8 aciertos sobre 10, mientras que el modelo student obtuvo 3. Esta diferencia es coherente con la literatura sobre destilaci&oacute;n, donde se espera una p&eacute;rdida moderada de precisi&oacute;n a cambio de una mayor eficiencia.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">En segundo lugar, analic&eacute; la velocidad de generaci&oacute;n mediante la m&eacute;trica de tokens por segundo. Aqu&iacute; el modelo destilado mostr&oacute; su principal ventaja: gener&oacute; texto m&aacute;s del doble de r&aacute;pido que el modelo original. Esta mejora confirma que la destilaci&oacute;n no solo reduce el tama&ntilde;o del modelo, sino que tambi&eacute;n optimiza su capacidad de respuesta, haci&eacute;ndolo m&aacute;s adecuado para entornos con recursos limitados o aplicaciones que requieren baja latencia.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">Finalmente, gener&eacute; gr&aacute;ficas comparativas que reflejan de forma visual y clara las diferencias entre ambos modelos, tanto en precisi&oacute;n como en velocidad. Estas representaciones permiten observar de un vistazo el impacto real de la destilaci&oacute;n y facilitan la interpretaci&oacute;n de los resultados.</span></p><p class="c2 c8"><span class="c1"></span></p><p class="c2"><span class="c1">En conjunto, el trabajo demuestra que la destilaci&oacute;n es una t&eacute;cnica eficaz para obtener modelos m&aacute;s ligeros y r&aacute;pidos, manteniendo un rendimiento razonable en tareas de comprensi&oacute;n. Aunque el modelo student no iguala al teacher en precisi&oacute;n, s&iacute; ofrece una mejora sustancial en eficiencia, lo que valida el proceso y confirma que la destilaci&oacute;n es una estrategia &uacute;til cuando se busca un equilibrio entre coste computacional y calidad del modelo.</span></p><p class="c2 c8"><span class="c4"></span></p></body></html>